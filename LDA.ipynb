{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "n_samples=50\n",
    "data_samples = dataset.data[:n_samples]\n",
    "\n",
    "n_features=300\n",
    "tmp = CountVectorizer(max_features=n_features,\n",
    "                                stop_words='english')\n",
    "corpus = tmp.fit_transform(data_samples)\n",
    "corpus.toarray().shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "class myLDA:\n",
    "    \n",
    "    def __init__(self, corpus, n_topics):\n",
    "        '''\n",
    "            Parameters\n",
    "            ----------\n",
    "            corpus: text set\n",
    "            n_topics: number of topic\n",
    "        '''\n",
    "        \n",
    "        self.Doc = corpus.toarray() # Documents\n",
    "        self.W = np.arange(self.Doc.shape[1]) # Words\n",
    "        self.n_doc = self.Doc.shape[0]  # num of docs\n",
    "        self.n_words = self.W.shape[0]  # num of words\n",
    "        self.n_topics = n_topics  # num of topics\n",
    "        # Dirichlet priors\n",
    "        self.alpha = np.ones(self.n_topics)\n",
    "        self.eta = np.ones(self.n_words)\n",
    "        # Z: word topic assignment  Pi: document topic distribution B: word topic distribution\n",
    "        self.Z = np.zeros(shape=[self.n_doc, self.n_words])\n",
    "        self.Pi = np.zeros([self.n_doc, self.n_topics])\n",
    "        self.B = np.zeros([self.n_topics, self.n_words])\n",
    "        self.init_parametres()\n",
    "\n",
    "    def init_parametres(self):\n",
    "        '''\n",
    "            Initialize Z， Pi, B\n",
    "        '''\n",
    "        for i in range(self.n_doc):\n",
    "            for j in range(self.n_words):\n",
    "                self.Z[i, j] = np.random.randint(self.n_topics)\n",
    "        for i in range(self.n_doc):\n",
    "            self.Pi[i] = np.random.dirichlet(self.alpha)\n",
    "        for i in range(self.n_topics):\n",
    "            self.B[i] = np.random.dirichlet(self.eta)\n",
    "            \n",
    "    def update(self, method=\"Gibbs_Sampling\", epoch=1, w=1, K=1):\n",
    "        '''\n",
    "            Update Z， Pi, B according to the article \"On Smoothing and Inference for Topic Models\"\n",
    "            The link is 'https://arxiv.org/pdf/1205.2662.pdf'\n",
    "        '''\n",
    "        start = time.time()\n",
    "        for it in tqdm.tqdm(range(epoch)):\n",
    "            eval(\"self.\" + method)(w,K)\n",
    "        end = time.time()\n",
    "        print(\"Time of training:\",end-start)\n",
    "        \n",
    "    def ML(self, w=1, K=1):\n",
    "        '''\n",
    "            ML ESTIMATION\n",
    "        '''\n",
    "        for i in range(self.n_doc):\n",
    "            for v in range(self.n_words):\n",
    "                p_iv = np.exp(np.log(self.Pi[i]) + np.log(self.B[:, self.Doc[i, v]]))\n",
    "                p_iv/= np.sum(p_iv)\n",
    "                self.Z[i, v] = p_iv.argmax() #update word topic assignment \n",
    "        for i in range(self.n_doc):\n",
    "            m = np.zeros(self.n_topics)\n",
    "            for k in range(self.n_topics):\n",
    "                m[k] = np.sum(self.Z[i] == k)\n",
    "            self.Pi[i, :] = m / sum(m) #update document topic distribution \n",
    "        for k in range(self.n_topics):\n",
    "            n = np.zeros(self.n_words)\n",
    "            for v in range(self.n_words):\n",
    "                for i in range(self.n_doc):\n",
    "                    for l in range(self.n_words):\n",
    "                        n[v] += (self.Doc[i, l] == v) and (self.Z[i, l] == k)\n",
    "            self.B[k, :] = n / sum(n) #update word topic distribution\n",
    "\n",
    "    def MAP(self, w=1, K=1):\n",
    "        '''\n",
    "            MAP ESTIMATION\n",
    "        '''\n",
    "        for i in range(self.n_doc):\n",
    "            m = np.zeros(self.n_topics)\n",
    "            for k in range(self.n_topics):\n",
    "                m[k] = np.sum(Z[i] == k)\n",
    "            self.Pi[i, :] = (m + self.alpha-1) / (sum(m) + K*self.alpha - K) #update document topic distribution  \n",
    "\n",
    "        for k in range(self.n_topics):\n",
    "            n = np.zeros(self.n_words)\n",
    "            for v in range(self.n_words):\n",
    "                for i in range(self.n_doc):\n",
    "                    for l in range(self.n_words):\n",
    "                        n[v] += (self.Doc[i, l] == v) and (self.Z[i, l] == k)\n",
    "            self.B[k, :] = (n + self.eta-1) / (sum(n) + w*self.eta - w) #update word topic distribution\n",
    "        for i in range(n_doc):\n",
    "            m = np.zeros(self.n_topics)\n",
    "            for k in range(self.n_topics):\n",
    "                m[k] = np.sum(self.Z[i] == k)\n",
    "            for v in range(self.n_words):\n",
    "                p_iv = Pi[i] * self.B[:, self.Doc[i, v]] * (sum(m) + K*self.alpha - K)\n",
    "                p_iv/= np.sum(p_iv)\n",
    "                self.Z[i, v] = p_iv.argmax() #update word topic assignment\n",
    "    \n",
    "    def VB(self, w=1, K=1):\n",
    "        '''\n",
    "            VARIATIONAL BAYES\n",
    "        '''\n",
    "        for i in range(self.n_doc):\n",
    "            m = np.zeros(self.n_topics)\n",
    "            for k in range(self.n_topics):\n",
    "                m[k] = np.sum(Z[i] == k)\n",
    "            self.Pi[i, :] = (m + self.alpha - 0.5) / (sum(m) + K*self.alpha - 0.5) #update document topic distribution  \n",
    "\n",
    "        for k in range(self.n_topics):\n",
    "            n = np.zeros(self.n_words)\n",
    "            for v in range(self.n_words):\n",
    "                for i in range(self.n_doc):\n",
    "                    for l in range(self.n_words):\n",
    "                        n[v] += (self.Doc[i, l] == v) and (self.Z[i, l] == k)\n",
    "            self.B[k, :] = (n + self.eta - 0.5) / (sum(n) + w*self.eta - 0.5) #update word topic distribution\n",
    "        for i in range(n_doc):\n",
    "            m = np.zeros(self.n_topics)\n",
    "            for k in range(self.n_topics):\n",
    "                m[k] = np.sum(self.Z[i] == k)\n",
    "            for v in range(self.n_words):\n",
    "                p_iv = Pi[i] * self.B[:, self.Doc[i, v]] * (sum(m) + K*self.alpha - 0.5)\n",
    "                p_iv/= np.sum(p_iv)\n",
    "                self.Z[i, v] = p_iv.argmax() #update word topic assignment   \n",
    "    \n",
    "    def CVB0(self, w=1, K=1):\n",
    "        '''\n",
    "            COLLAPSED VARIATIONAL BAYES\n",
    "        '''\n",
    "        for i in range(self.n_doc):\n",
    "            m = np.zeros(self.n_topics)\n",
    "            for k in range(self.n_topics):\n",
    "                m[k] = np.sum(Z[i] == k)\n",
    "            self.Pi[i, :] = (m + self.alpha - 0.5) / (sum(m) + K*self.alpha - 0.5) #update document topic distribution  \n",
    "\n",
    "        for k in range(self.n_topics):\n",
    "            n = np.zeros(self.n_words)\n",
    "            for v in range(self.n_words):\n",
    "                for i in range(self.n_doc):\n",
    "                    for l in range(self.n_words):\n",
    "                        n[v] += (self.Doc[i, l] == v) and (self.Z[i, l] == k)\n",
    "            self.B[k, :] = (n + self.eta - 0.5) / (sum(n) + w*self.eta - 0.5) #update word topic distribution\n",
    "        for i in range(self.n_doc):\n",
    "            for v in range(self.n_words):\n",
    "                p_iv = np.exp(np.log(self.Pi[i]) + np.log(self.B[:, self.Doc[i, v]]))\n",
    "                p_iv/= np.sum(p_iv)\n",
    "                self.Z[i, v] = np.random.multinomial(1, p_iv).argmax() #update word topic assignment \n",
    "                \n",
    "    def Gibbs_Sampling(self, w=1, K=1):\n",
    "        '''\n",
    "            COLLAPSED GIBBS SAMPLING\n",
    "        '''\n",
    "        for i in range(self.n_doc):\n",
    "            for v in range(self.n_words):\n",
    "                p_iv = np.exp(np.log(self.Pi[i]) + np.log(self.B[:, self.Doc[i, v]]))\n",
    "                p_iv/= np.sum(p_iv)\n",
    "                self.Z[i, v] = np.random.multinomial(1, p_iv).argmax() #update word topic assignment \n",
    "        for i in range(self.n_doc):\n",
    "            m = np.zeros(self.n_topics)\n",
    "            for k in range(self.n_topics):\n",
    "                m[k] = np.sum(self.Z[i] == k)\n",
    "            self.Pi[i, :] = np.random.dirichlet(self.alpha + m) #update document topic distribution \n",
    "        for k in range(n_topics):\n",
    "            n = np.zeros(n_words)\n",
    "            for v in range(self.n_words):\n",
    "                for i in range(self.n_doc):\n",
    "                    for l in range(self.n_words):\n",
    "                        n[v] += (self.Doc[i, l] == v) and (self.Z[i, l] == k)\n",
    "            self.B[k, :] = np.random.dirichlet(self.eta + n) #update word topic distribution\n",
    "    \n",
    "    def get_w_t(self):\n",
    "        '''\n",
    "            Get word topic assignment\n",
    "        '''\n",
    "        return self.Z\n",
    "    \n",
    "    def get_t_d(self):\n",
    "        '''\n",
    "            Get document topic distribution\n",
    "        '''\n",
    "        return self.Pi\n",
    "    \n",
    "    def get_w_d(self):\n",
    "        '''\n",
    "            Get word topic distribution\n",
    "        '''\n",
    "        return self.B\n",
    "    \n",
    "    def evaluate(self):\n",
    "        '''\n",
    "            Perplexity\n",
    "        '''\n",
    "        epision = 1\n",
    "        P = np.log(np.dot(self.Pi, self.B) + epision)\n",
    "        s = sum(sum(P))\n",
    "        t = -(s / (self.n_topics * self.n_words))\n",
    "        return np.exp(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:52<07:51, 52.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A/home/gjx/.local/lib/python3.6/site-packages/ipykernel_launcher.py:58: RuntimeWarning: divide by zero encountered in log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [01:51<07:14, 54.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [02:58<06:47, 58.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [04:20<06:31, 65.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [06:56<07:43, 92.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [07:56<05:31, 82.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [09:03<03:54, 78.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [11:20<03:11, 95.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [13:19<01:42, 102.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [15:19<00:00, 107.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of training: 919.029703617096\n",
      "Document topic distribution:\n",
      " [[0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.007 0.    0.    0.    0.993]\n",
      " [0.35  0.43  0.    0.22  0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.997 0.    0.    0.    0.003]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.013 0.    0.977 0.01  0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.01  0.99  0.    0.    0.   ]\n",
      " [0.007 0.987 0.    0.007 0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    0.99  0.01  0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.01  0.    0.99  0.    0.   ]\n",
      " [0.    0.99  0.    0.007 0.003]\n",
      " [0.003 0.    0.997 0.    0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    0.997 0.003 0.    0.   ]\n",
      " [0.    0.    0.    0.    1.   ]\n",
      " [0.933 0.    0.    0.067 0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    0.997 0.003 0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    0.927 0.    0.073 0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.   ]]\n",
      "Model score:\n",
      " 0.9763225504953722\n"
     ]
    }
   ],
   "source": [
    "model1 = myLDA(corpus,5)\n",
    "model1.update(method =\"ML\", epoch = 10)\n",
    "print(\"Document topic distribution:\\n\",np.round(model1.get_t_d(), decimals=3))\n",
    "print(\"Model score:\\n\",model1.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [01:58<17:44, 118.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [03:56<15:46, 118.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [05:55<13:49, 118.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [06:54<10:03, 100.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [07:50<07:15, 87.13s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [08:41<05:05, 76.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [09:32<03:26, 68.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [10:26<02:08, 64.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [11:18<01:00, 60.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [12:09<00:00, 57.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of training: 729.5461647510529\n",
      "Document topic distribution:\n",
      " [[0.193 0.243 0.173 0.193 0.197]\n",
      " [0.253 0.183 0.193 0.223 0.147]\n",
      " [0.183 0.183 0.167 0.257 0.21 ]\n",
      " [0.21  0.21  0.18  0.197 0.203]\n",
      " [0.193 0.197 0.247 0.167 0.197]\n",
      " [0.24  0.18  0.177 0.217 0.187]\n",
      " [0.233 0.213 0.193 0.183 0.177]\n",
      " [0.213 0.2   0.22  0.203 0.163]\n",
      " [0.213 0.18  0.213 0.223 0.17 ]\n",
      " [0.177 0.203 0.2   0.187 0.233]\n",
      " [0.263 0.18  0.207 0.18  0.17 ]\n",
      " [0.227 0.243 0.203 0.19  0.137]\n",
      " [0.193 0.18  0.2   0.197 0.23 ]\n",
      " [0.18  0.177 0.28  0.18  0.183]\n",
      " [0.243 0.17  0.15  0.207 0.23 ]\n",
      " [0.193 0.163 0.227 0.187 0.23 ]\n",
      " [0.177 0.16  0.24  0.227 0.197]\n",
      " [0.24  0.19  0.203 0.197 0.17 ]\n",
      " [0.223 0.193 0.213 0.173 0.197]\n",
      " [0.153 0.217 0.2   0.197 0.233]\n",
      " [0.233 0.223 0.177 0.177 0.19 ]\n",
      " [0.177 0.24  0.2   0.197 0.187]\n",
      " [0.233 0.177 0.167 0.223 0.2  ]\n",
      " [0.207 0.19  0.197 0.187 0.22 ]\n",
      " [0.207 0.2   0.227 0.213 0.153]\n",
      " [0.187 0.17  0.227 0.23  0.187]\n",
      " [0.18  0.237 0.22  0.193 0.17 ]\n",
      " [0.213 0.183 0.183 0.197 0.223]\n",
      " [0.2   0.197 0.2   0.213 0.19 ]\n",
      " [0.233 0.157 0.217 0.197 0.197]\n",
      " [0.187 0.183 0.2   0.21  0.22 ]\n",
      " [0.183 0.197 0.203 0.217 0.2  ]\n",
      " [0.21  0.193 0.203 0.187 0.207]\n",
      " [0.207 0.213 0.16  0.187 0.233]\n",
      " [0.173 0.193 0.17  0.23  0.233]\n",
      " [0.183 0.2   0.223 0.19  0.203]\n",
      " [0.18  0.187 0.21  0.187 0.237]\n",
      " [0.22  0.157 0.21  0.22  0.193]\n",
      " [0.203 0.17  0.213 0.207 0.207]\n",
      " [0.187 0.19  0.203 0.217 0.203]\n",
      " [0.193 0.223 0.18  0.21  0.193]\n",
      " [0.16  0.21  0.197 0.213 0.22 ]\n",
      " [0.21  0.257 0.193 0.183 0.157]\n",
      " [0.22  0.19  0.183 0.19  0.217]\n",
      " [0.17  0.217 0.22  0.21  0.183]\n",
      " [0.2   0.163 0.17  0.24  0.227]\n",
      " [0.19  0.243 0.237 0.143 0.187]\n",
      " [0.23  0.177 0.21  0.16  0.223]\n",
      " [0.17  0.217 0.207 0.21  0.197]\n",
      " [0.22  0.17  0.203 0.19  0.217]]\n",
      "Model score:\n",
      " 0.9763521282610481\n"
     ]
    }
   ],
   "source": [
    "model2 = myLDA(corpus,5)\n",
    "model2.update(method =\"MAP\", epoch = 10)\n",
    "print(\"Document topic distribution:\\n\",np.round(model2.get_t_d(), decimals=3))\n",
    "print(\"Model score:\\n\",model2.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:57<08:39, 57.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [01:52<07:34, 56.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [02:44<06:27, 55.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [03:36<05:27, 54.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [04:28<04:27, 53.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [05:21<03:33, 53.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [06:17<02:42, 54.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [07:12<01:48, 54.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [08:08<00:54, 54.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [09:05<00:00, 55.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of training: 545.8619277477264\n",
      "Document topic distribution:\n",
      " [[0.195 0.245 0.175 0.195 0.198]\n",
      " [0.255 0.185 0.195 0.225 0.148]\n",
      " [0.185 0.185 0.168 0.258 0.211]\n",
      " [0.211 0.211 0.181 0.198 0.205]\n",
      " [0.195 0.198 0.248 0.168 0.198]\n",
      " [0.241 0.181 0.178 0.218 0.188]\n",
      " [0.235 0.215 0.195 0.185 0.178]\n",
      " [0.215 0.201 0.221 0.205 0.165]\n",
      " [0.215 0.181 0.215 0.225 0.171]\n",
      " [0.178 0.205 0.201 0.188 0.235]\n",
      " [0.265 0.181 0.208 0.181 0.171]\n",
      " [0.228 0.245 0.205 0.191 0.138]\n",
      " [0.195 0.181 0.201 0.198 0.231]\n",
      " [0.181 0.178 0.281 0.181 0.185]\n",
      " [0.245 0.171 0.151 0.208 0.231]\n",
      " [0.195 0.165 0.228 0.188 0.231]\n",
      " [0.178 0.161 0.241 0.228 0.198]\n",
      " [0.241 0.191 0.205 0.198 0.171]\n",
      " [0.225 0.195 0.215 0.175 0.198]\n",
      " [0.155 0.218 0.201 0.198 0.235]\n",
      " [0.235 0.225 0.178 0.178 0.191]\n",
      " [0.178 0.241 0.201 0.198 0.188]\n",
      " [0.235 0.178 0.168 0.225 0.201]\n",
      " [0.208 0.191 0.198 0.188 0.221]\n",
      " [0.208 0.201 0.228 0.215 0.155]\n",
      " [0.188 0.171 0.228 0.231 0.188]\n",
      " [0.181 0.238 0.221 0.195 0.171]\n",
      " [0.215 0.185 0.185 0.198 0.225]\n",
      " [0.201 0.198 0.201 0.215 0.191]\n",
      " [0.235 0.158 0.218 0.198 0.198]\n",
      " [0.188 0.185 0.201 0.211 0.221]\n",
      " [0.185 0.198 0.205 0.218 0.201]\n",
      " [0.211 0.195 0.205 0.188 0.208]\n",
      " [0.208 0.215 0.161 0.188 0.235]\n",
      " [0.175 0.195 0.171 0.231 0.235]\n",
      " [0.185 0.201 0.225 0.191 0.205]\n",
      " [0.181 0.188 0.211 0.188 0.238]\n",
      " [0.221 0.158 0.211 0.221 0.195]\n",
      " [0.205 0.171 0.215 0.208 0.208]\n",
      " [0.188 0.191 0.205 0.218 0.205]\n",
      " [0.195 0.225 0.181 0.211 0.195]\n",
      " [0.161 0.211 0.198 0.215 0.221]\n",
      " [0.211 0.258 0.195 0.185 0.158]\n",
      " [0.221 0.191 0.185 0.191 0.218]\n",
      " [0.171 0.218 0.221 0.211 0.185]\n",
      " [0.201 0.165 0.171 0.241 0.228]\n",
      " [0.191 0.245 0.238 0.145 0.188]\n",
      " [0.231 0.178 0.211 0.161 0.225]\n",
      " [0.171 0.218 0.208 0.211 0.198]\n",
      " [0.221 0.171 0.205 0.191 0.218]]\n",
      "Model score:\n",
      " 0.9745151283425427\n"
     ]
    }
   ],
   "source": [
    "model3 = myLDA(corpus,5)\n",
    "model3.update(method =\"VB\", epoch = 10)\n",
    "print(\"Document topic distribution:\\n\",np.round(model3.get_t_d(), decimals=3))\n",
    "print(\"Model score:\\n\",model3.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:53<07:58, 53.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [01:45<07:02, 52.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [02:38<06:10, 52.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [03:30<05:16, 52.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [04:22<04:21, 52.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [05:13<03:28, 52.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [06:08<02:38, 52.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [07:07<01:49, 54.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [08:04<00:55, 55.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [08:55<00:00, 54.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of training: 535.4612009525299\n",
      "Document topic distribution:\n",
      " [[0.195 0.245 0.175 0.195 0.198]\n",
      " [0.255 0.185 0.195 0.225 0.148]\n",
      " [0.185 0.185 0.168 0.258 0.211]\n",
      " [0.211 0.211 0.181 0.198 0.205]\n",
      " [0.195 0.198 0.248 0.168 0.198]\n",
      " [0.241 0.181 0.178 0.218 0.188]\n",
      " [0.235 0.215 0.195 0.185 0.178]\n",
      " [0.215 0.201 0.221 0.205 0.165]\n",
      " [0.215 0.181 0.215 0.225 0.171]\n",
      " [0.178 0.205 0.201 0.188 0.235]\n",
      " [0.265 0.181 0.208 0.181 0.171]\n",
      " [0.228 0.245 0.205 0.191 0.138]\n",
      " [0.195 0.181 0.201 0.198 0.231]\n",
      " [0.181 0.178 0.281 0.181 0.185]\n",
      " [0.245 0.171 0.151 0.208 0.231]\n",
      " [0.195 0.165 0.228 0.188 0.231]\n",
      " [0.178 0.161 0.241 0.228 0.198]\n",
      " [0.241 0.191 0.205 0.198 0.171]\n",
      " [0.225 0.195 0.215 0.175 0.198]\n",
      " [0.155 0.218 0.201 0.198 0.235]\n",
      " [0.235 0.225 0.178 0.178 0.191]\n",
      " [0.178 0.241 0.201 0.198 0.188]\n",
      " [0.235 0.178 0.168 0.225 0.201]\n",
      " [0.208 0.191 0.198 0.188 0.221]\n",
      " [0.208 0.201 0.228 0.215 0.155]\n",
      " [0.188 0.171 0.228 0.231 0.188]\n",
      " [0.181 0.238 0.221 0.195 0.171]\n",
      " [0.215 0.185 0.185 0.198 0.225]\n",
      " [0.201 0.198 0.201 0.215 0.191]\n",
      " [0.235 0.158 0.218 0.198 0.198]\n",
      " [0.188 0.185 0.201 0.211 0.221]\n",
      " [0.185 0.198 0.205 0.218 0.201]\n",
      " [0.211 0.195 0.205 0.188 0.208]\n",
      " [0.208 0.215 0.161 0.188 0.235]\n",
      " [0.175 0.195 0.171 0.231 0.235]\n",
      " [0.185 0.201 0.225 0.191 0.205]\n",
      " [0.181 0.188 0.211 0.188 0.238]\n",
      " [0.221 0.158 0.211 0.221 0.195]\n",
      " [0.205 0.171 0.215 0.208 0.208]\n",
      " [0.188 0.191 0.205 0.218 0.205]\n",
      " [0.195 0.225 0.181 0.211 0.195]\n",
      " [0.161 0.211 0.198 0.215 0.221]\n",
      " [0.211 0.258 0.195 0.185 0.158]\n",
      " [0.221 0.191 0.185 0.191 0.218]\n",
      " [0.171 0.218 0.221 0.211 0.185]\n",
      " [0.201 0.165 0.171 0.241 0.228]\n",
      " [0.191 0.245 0.238 0.145 0.188]\n",
      " [0.231 0.178 0.211 0.161 0.225]\n",
      " [0.171 0.218 0.208 0.211 0.198]\n",
      " [0.221 0.171 0.205 0.191 0.218]]\n",
      "Model score:\n",
      " 0.9745674165718056\n"
     ]
    }
   ],
   "source": [
    "model4 = myLDA(corpus,5)\n",
    "model4.update(method =\"CVB0\", epoch = 10)\n",
    "print(\"Document topic distribution:\\n\",np.round(model4.get_t_d(), decimals=3))\n",
    "print(\"Model score:\\n\",model4.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:53<07:59, 53.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [01:44<07:01, 52.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [02:42<06:18, 54.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [03:34<05:21, 53.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [04:29<04:30, 54.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [05:23<03:36, 54.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [06:16<02:41, 53.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [07:09<01:47, 53.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [08:03<00:53, 53.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [08:53<00:00, 52.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of training: 533.9834718704224\n",
      "Document topic distribution:\n",
      " [[0.236 0.755 0.007 0.    0.002]\n",
      " [0.171 0.81  0.008 0.003 0.008]\n",
      " [0.012 0.978 0.003 0.003 0.004]\n",
      " [0.476 0.5   0.003 0.007 0.015]\n",
      " [0.224 0.764 0.007 0.002 0.003]\n",
      " [0.457 0.532 0.008 0.002 0.001]\n",
      " [0.111 0.884 0.001 0.003 0.   ]\n",
      " [0.368 0.624 0.002 0.001 0.005]\n",
      " [0.224 0.771 0.003 0.002 0.001]\n",
      " [0.601 0.379 0.003 0.001 0.017]\n",
      " [0.118 0.028 0.849 0.003 0.003]\n",
      " [0.21  0.777 0.003 0.006 0.003]\n",
      " [0.139 0.851 0.001 0.004 0.005]\n",
      " [0.271 0.712 0.005 0.    0.011]\n",
      " [0.644 0.348 0.002 0.005 0.   ]\n",
      " [0.051 0.938 0.002 0.005 0.004]\n",
      " [0.006 0.988 0.003 0.001 0.001]\n",
      " [0.633 0.331 0.026 0.006 0.004]\n",
      " [0.183 0.786 0.014 0.002 0.014]\n",
      " [0.34  0.646 0.011 0.003 0.   ]\n",
      " [0.902 0.083 0.004 0.005 0.006]\n",
      " [0.21  0.769 0.001 0.006 0.014]\n",
      " [0.016 0.965 0.014 0.002 0.003]\n",
      " [0.004 0.971 0.007 0.004 0.014]\n",
      " [0.29  0.696 0.01  0.001 0.003]\n",
      " [0.439 0.552 0.004 0.004 0.002]\n",
      " [0.281 0.713 0.002 0.    0.004]\n",
      " [0.012 0.971 0.014 0.003 0.   ]\n",
      " [0.247 0.681 0.065 0.001 0.006]\n",
      " [0.107 0.888 0.001 0.003 0.001]\n",
      " [0.152 0.823 0.004 0.005 0.016]\n",
      " [0.081 0.914 0.    0.004 0.001]\n",
      " [0.197 0.8   0.001 0.001 0.001]\n",
      " [0.036 0.957 0.005 0.    0.002]\n",
      " [0.251 0.715 0.016 0.018 0.001]\n",
      " [0.017 0.979 0.003 0.001 0.   ]\n",
      " [0.325 0.666 0.003 0.    0.006]\n",
      " [0.047 0.933 0.003 0.002 0.015]\n",
      " [0.139 0.854 0.001 0.001 0.006]\n",
      " [0.177 0.819 0.001 0.003 0.   ]\n",
      " [0.041 0.919 0.007 0.029 0.004]\n",
      " [0.454 0.539 0.006 0.    0.   ]\n",
      " [0.791 0.196 0.003 0.005 0.005]\n",
      " [0.147 0.82  0.013 0.019 0.001]\n",
      " [0.021 0.975 0.002 0.    0.002]\n",
      " [0.338 0.637 0.013 0.008 0.004]\n",
      " [0.135 0.852 0.006 0.005 0.002]\n",
      " [0.117 0.876 0.005 0.001 0.001]\n",
      " [0.616 0.362 0.003 0.009 0.009]\n",
      " [0.404 0.56  0.022 0.003 0.01 ]]\n",
      "Model score:\n",
      " 0.9756373156619539\n"
     ]
    }
   ],
   "source": [
    "model5 = myLDA(corpus,5)\n",
    "model5.update(method =\"Gibbs_Sampling\", epoch = 10)\n",
    "print(\"Document topic distribution:\\n\",np.round(model5.get_t_d(), decimals=3))\n",
    "print(\"Model score:\\n\",model5.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
